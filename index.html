<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />
    <title> Pei Lin </title>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4S2YH6VQ1Y"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4S2YH6VQ1Y');
</script>

<body>
    <div style="width:1000px;margin: 0px auto;">
        <header id="header" width="400px" style="display:flex;justify-content: space-around;">
            <a href="#profile-intro">Home</a>
            <a href="#updates">Updates</a>
            <a href="#research">Research</a>
            <!-- <a href="#projects">Projects</a> -->
        </header>
        <div id="profile">
            <div id="profile-pic">
                <img src="assets/images/profile.jpg" />
            </div>
            <div id="profile-intro">
                <div id="profile-name">Pei Lin 林沛</div>
                <div id="profile-email">linpei@bigai.ai<br> 
                       linpei2024@shanghaitech.edu.cn 
                </div>
                <p>
                    I'm a sencond-year PhD candidate in Computer Science at Beijing Institute for General Artificial Intelligence (BIGAI) and ShanghaiTech University. 
                    I'm fortunate to be advised by Prof. <a href="https://sites.google.com/g.ucla.edu/zyjiao/home">Ziyuan Jiao</a> and Prof. <a href="https://rim-laboratory.github.io/">Chenxi Xiao</a>.
                </p>
                <p>
                    My current research centers on dexterous robotic hands equipped with tactile perception. 
                    I aim to enable these dexterous robotic hands to perform more complex and higher-level maneuvers by endowing them with tactile capabilities.
                </p>
                <p>
                    In my previous research, I have primarily focused on human body motion,
                    particularly interactive hand motion, which has greatly contributed to my current work. 
                    I feel incredibly fortunate to have had the guidance of Prof. <a href="http://www.xu-lan.com/">Lan Xu</a>, Prof. <a href="https://www.yu-jingyi.com/">Jingyi Yi</a> and  Prof. <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>.
                </p>
                
                <div>
                    <a href="https://scholar.google.com/citations?user=wR-6tm4AAAAJ&hl=zh-CN&authuser=1">
                      Google Scholar
                    </a>
                    </a>
                    /
                    <a href="https://github.com/PeiLin-666">
                        Github
                    </a>
                    </a>
                </div>
            </div>
            <div style="clear: both;"></div>
        </div>
        <div class="section" id="updates">
            <h1>Updates</h1>
            <ul>
                <li> <b>Jan 2026</b> <a href="https://peilin-666.github.io/projects/DexMove/"><strong>DexMove</strong></a> is accdepted by ICLR 2026! See you in Rio de Janeiro!
                <li> <b>June 2025</b> <a href="https://ieeexplore.ieee.org/document/11246144"><strong>R-Tac0</strong></a> is accdepted by IROS 2025! See you in Hangzhou!
                <li> <b>April 2025</b> <a href="https://peilin-666.github.io/projects/PP-Tac/"><strong>PP-Tac</strong></a> is accdepted by RSS 2025! The harware has been released! See you in LA!
                <li> <b>Mar 2025</b> <a href="https://peilin-666.github.io/projects/PP-Tac/"><strong>PP-Tac</strong></a> is accdepted to ICLR 7th Robot Learning Workshop as an oral paper, see you in Singapore!
                <li> <b>Dec 2024</b> One papers accepted to AAAI 2025
                <li> <b>Jul 2024</b> I started my PhD at BIGAI
            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="research">
            <h1>Research</h1>

            <div>
              <a href="https://peilin-666.github.io/projects/TaF_VLA/" style="height: 14em;" class="research-thumb">
                  <img width="320" src="assets/images/TaF.png" alt="">
              </a>
              <a href="https://peilin-666.github.io/projects/TaF_VLA" class="research-proj-title">
                TaF-VLA: Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation
              </a>
              <p>
                  <a style="color:#000;">Yuzhe Huang</a>*,
                  <b>Pei Lin</b>*,
                  <a style="color:#000;">Wanlin Li</a>*,
                  <a style="color:#000;">Daohan Li</a>,
                  <a style="color:#000;">Jiajun Li</a>,
                  <a style="color:#000;">Jiaming Jiang</a>,
                  <a style="color:#000;">Chenxi Xiao</a>†,
                  <a style="color:#000;">Ziyuan Jiao</a>†
                  <br>
                  <a href="https://peilin-666.github.io/projects/TaF_VLA/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/abs/2601.20321">Arxiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://github.com/mrHuangyz/TaF-VLA">Code</a>
              </p>
            </div>


            <div>
              <a href="https://peilin-666.github.io/projects/DexMove/" style="height: 14em;" class="research-thumb">
                  <img width="320" src="assets/images/DexMove.png" alt="">
              </a>
              <a href="https://peilin-666.github.io/projects/DexMove/" class="research-proj-title">
                DexMove: Learning Tactile-Guided Non-Prehensile Manipulation with Dexterous Hands
              </a>
              <p>
                  <b>Pei Lin</b>*,
                  <a style="color:#000;">Yuzhe Huang</a>*,
                  <a style="color:#000;">Wanlin Li</a>*,
                  <a style="color:#000;">Chenxi Xiao</a>†,
                  <a style="color:#000;">Ziyuan Jiao</a>†
                  <br>The Fourteenth International Conference on Learning Representations (<strong > ICLR 2026</strong>)<br>
                  <a href="https://peilin-666.github.io/projects/DexMove/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://openreview.net/pdf?id=dT3ZciXvNX">Paper</a>
              </p>
            </div>

            
            <div>
                <a style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/R-FTact.png" alt="">
                </a>
                <a  class="research-proj-title">
                    R-Tac0: A Rounded High-Frequency Transferable Monochrome Vision-based Tactile Sensor for Shape Reconstruction
                </a>
                <p>
                    <a style="color:#000;">Wanlin Li</a>*,
                    <b>Pei Lin</b>*,
                    <a style="color:#000;">Meng Wang</a>,
                    <a style="color:#000;">Chenxi Xiao</a>,
                    <a style="color:#000;">Kaspar Althoefer</a>,
                    <a style="color:#000;">Yao Su</a>,
                    <a style="color:#000;">Ziyuan Jiao</a>,
                    <a style="color:#000;">Hangxin Liu</a>
                    <br>The IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong> IROS 2025</strong> )<br>
                    <a href="https://ieeexplore.ieee.org/document/11246144">Paper</a>
                </p>
            </div>

            <div>
              <a href="https://peilin-666.github.io/projects/PP-Tac/" style="height: 14em;" class="research-thumb">
                  <img width="320" src="assets/images/PP_Tac.png" alt="">
              </a>
              <a href="https://peilin-666.github.io/projects/PP-Tac/" class="research-proj-title">
                PP-Tac: Paper Picking Using Omnidirectional Tactile Feedback in Dexterous Robotic Hands
              </a>
              <p>
                  <b>Pei Lin</b>*,
                  <a style="color:#000;">Yuzhe Huang</a>*,
                  <a style="color:#000;">Wanlin Li</a>*,
                  <a style="color:#000;">Jianpeng Ma</a>,
                  <a style="color:#000;">Chenxi Xiao</a>†,
                  <a style="color:#000;">Ziyuan Jiao</a>†
                  <br>Robotics: Science and Systems 2025 (<strong > RSS 2025</strong>)
                  <br>7th Robot Learning Workshop in ICLR 2025, <strong > oral</strong><br>
                  <a href="https://peilin-666.github.io/projects/PP-Tac/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://roboticsconference.org/program/papers/56/">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://arxiv.org/abs/2504.16649">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://github.com/bigai-ai/PP-Tac/tree/main">Code
              </p>
            </div>

          


            <div>
                <a href="https://handdiffuse.github.io" style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/HandDiffuse.png" alt="">
                </a>
                <a href="https://handdiffuse.github.io" class="research-proj-title">
                    HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models
                </a>
                <p>
                    <b>Pei Lin</b>*,
                    <a style="color:#000;">Sihang Xu</a>,
                    <a style="color:#000;">Hongdi Yang</a>,
                    <a style="color:#000;">Yiran Liu</a>,
                    <a style="color:#000;">Xin Chen</a>,
                    <a style="color:#000;">Jingya Wang</a>,
                    <a style="color:#000;">Jingyi Yu</a>,
                    <a style="color:#000;">Lan Xu</a>
                    <br>Thirty-Ninth AAAI Conference on Artificial Intelligence (<strong> AAAI 2025</strong>)<br>
                    <a href="https://handdiffuse.github.io">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/abs/2312.04867">ArXiv</a>
                    <!-- &nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                    <!-- <a href="https://handdiffuse.github.io">Dataset(Coming Soon) -->
                </p>
            </div>

            <div>
                <a href="https://zhaofuq.github.io/humannerf/" style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/humannerf.png" alt="">
                </a>
                <a href="https://zhaofuq.github.io/humannerf/" class="research-proj-title">
                    HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs
                </a>
                <p>
                    <a style="color:#000;">Fuqiang Zhao</a>,
                    <a style="color:#000;">Wei Yang</a>,
                    <a style="color:#000;">Jiakai Zhang</a>,
                    <b>Pei Lin</b>,
                    <a style="color:#000;">Yingliang Zhang</a>,
                    <a style="color:#000;">Jingyi Yu</a>,
                    <a style="color:#000;">Lan Xu</a>
                    <br>Conference on Computer Vision and Pattern Recognition (<strong> CVPR 2022</strong>)<br>
                    <a href="https://zhaofuq.github.io/humannerf/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/pdf/2112.02789.pdf">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://zhaofuq.github.io/humannerf/">Code
                </p>
            </div>


            <div>
                <a href="https://nowheretrix.github.io/neuralhumanFVV/" style="height: 14em;" class="research-thumb">
                    <video width="320" height="auto" autoplay loop muted>
                        <source src="assets/images/NeuralHumanFVV.mp4" type="video/mp4">
                    </video>
                </a>
                <a href="https://nowheretrix.github.io/neuralhumanFVV/" class="research-proj-title">
                    NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras
                </a>
                <p>
                    <a style="color:#000;">Xin Suo</a>,
                    <a style="color:#000;">Yuheng Jiang</a>,
                    <b>Pei Lin</b>,
                    <a style="color:#000;">Yingliang Zhang</a>,
                    <a style="color:#000;">Minye Wu</a>,
                    <a style="color:#000;">Kaiwen Guo</a>,
                    <a style="color:#000;">Lan Xu</a>
                    <br>Conference on Computer Vision and Pattern Recognition (<strong> CVPR 2021</strong>)<br>
                    <a href="https://nowheretrix.github.io/neuralhumanFVV/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Suo_NeuralHumanFVV_Real-Time_Neural_Volumetric_Human_Performance_Rendering_Using_RGB_Cameras_CVPR_2021_paper.html">ArXiv</a>
                </p>
            </div>



            <div>
                <a href="https://sunshinnnn.github.io/HOI-FVV" style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/HOI.png" alt="">
                </a>
                <a href="https://umi-gripper.github.io/" class="research-proj-title">
                    Neural Free-Viewpoint Performance Rendering under Complex Human-object Interactions
                </a>
                <p>
                    <a style="color:#000;">Guoxing Sun</a>,
                    <a style="color:#000;">Xin Chen</a>,
                    <a style="color:#000;">Yizhang Chen</a>,
                    <a style="color:#000;">Anqi Pang</a>,
                    <b>Pei Lin</b>,
                    <a style="color:#000;">Yuheng Jiang</a>,
                    <a style="color:#000;">Lan Xu</a>,
                    <a style="color:#000;">Jingya Wang</a>,
                    <a style="color:#000;">Jingyi Yu</a>
                    <br>ACM Multimedia (<strong> ACM MM 2021 oral </strong>)<br>
                    <a href="https://sunshinnnn.github.io/HOI-FVV">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/abs/2312.04867">ArXiv</a>
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>



</body>
<footer style="text-align: center;">
  © 2025 Pei Lin. All rights reserved.<br>
    Template designed by <a href="https://cheng-chi.github.io" target="_blank">Cheng Chi</a>.
</footer>
</html>
