<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DexMove: Learning Tactile-Guided Non-Prehensile Manipulation with Dexterous Hands">
  <meta name="keywords" content="DexMove, Tactile, Dexterous Hands, Robotics, Non-Prehensile Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DexMove: Learning Tactile-Guided Non-Prehensile Manipulation with Dexterous Hands</title>

	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-R2K96FXEJD"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-E1D3L6JXGY');
	</script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://peilin-666.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://peilin-666.github.io/projects/PP-Tac/">
            PP-Tac
          </a>
          <a class="navbar-item" href="https://ieeexplore.ieee.org/document/11246144">
            R-Tac0
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="color: #f47514;">DexMove</span>: Learning Tactile-Guided <br> Non-Prehensile Manipulation with Dexterous Hands</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://peilin-666.github.io/">Pei Lin</a><sup>*,1,2</sup>,</span>
            <span class="author-block">
              Yuzhe Huang<sup>*,1,3</sup>,</span>
            <span class="author-block">
              <a href="https://mrliwanlin.github.io/">Wanlin Li</a><sup>*,1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=Qhiy3doAAAAJ">Chenxi Xiao</a><sup>†,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YfWhJZoAAAAJ&hl=zh-CN&oi=ao">Ziyuan Jiao</a><sup>†,1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <br>
            <span>* Denotes equal contribution; † Co-corresponding authors</span>
            <span class="author-block"><sup>1</sup>Beijing Institute for General Artificial Intelligence,</span>
            <span class="author-block"><sup>2</sup>ShanghaiTech University, </span>
            <span class="author-block"><sup>3</sup>Beihang University</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=dT3ZciXvNX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=dT3ZciXvNX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://peilin-666.github.io/projects/DexMove/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DexMove</span> can learn tactile-guided non-prehensile manipulation skills with dexterous hands from
        demonstrations
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-app1">
          <video poster="" id="app1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/application1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-app2">
          <video poster="" id="app2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/application2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-app3">
          <video poster="" id="app3" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/application3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-comp1">
          <video poster="" id="comp1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comparison1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-comp2">
          <video poster="" id="comp2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comparison2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-comp3">
          <video poster="" id="comp3" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comparison3.mp4"
                    type="video/mp4">
          </video>
        </div>
       
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Non-prehensile manipulation offers a robust alternative to traditional pick-and-place methods for object repositioning. 
            However, learning such skills with dexterous, multi-fingered hands remains largely unexplored, 
            leaving their potential for stable and efficient manipulation underutilized. 
            Progress has been limited by the lack of large-scale, contact-aware non-prehensile datasets for dexterous hands 
            and the absence of wrist-finger control policies. 
          </p>
          <p>
            To bridge these gaps, we present <span class="dnerf">DexMove</span>, a tactile-guided non-prehensile manipulation 
            framework for dexterous hands.  <span class="dnerf">DexMove</span> combines a scalable simulation pipeline that generates physically 
            plausible wrist-finger trajectories with a wearable device, which captures multi-finger contact data from human demonstrations using vision-based tactile sensors. 
            Using these data, we train a flow-based policy that enables real-time, synergistic wrist-finger control for robust non-prehensile manipulation of diverse tabletop objects. 
          </p>
          <p> 
            In real-world experiments,  <span class="dnerf">DexMove</span> successfully manipulated six objects of varying shapes and materials, achieving a 77.8% success rate. 
            Our method outperforms ablated baselines by 36.6% and improves efficiency by nearly 300%. Furthermore, the learned policy generalizes to language-conditioned, long-horizon tasks such as object sorting and desktop tidying.
          </p>

        </div>
      </div>
    </div>
    
    <!-- insert a image -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <figure class="image">
      <img src="./static/images/teaser_overview.png"
           alt="DexMove Overview"
           style="width: 100%; height: auto;"/>
    </figure>
    <p class="has-text-justified" style="margin-top: 1rem; font-style: italic;">
      <strong>Figure 1:</strong> Overview of <span class="dnerf">DexMove</span>. 
      The framework integrates synthetic non-prehensile manipulation trajectories and human-demonstrated tactile data to train a flow-matching policy for dexterous hands. The learned policy generalizes across diverse objects, surface frictions, and various language-conditioned tasks such as tidying.
    </p>
  </div>
</div>

    <!-- Paper video.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Data Collection</h2>
    <div class="columns is-centered">
      
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Force-aware Trajectory Synthesis </h2>
          
          <figure class="image" style="margin-bottom: 0;">
            <img src="./static/images/trajectory_synthesis.png"
                alt="Trajectory synthesis overview"
                style="width: 100%; height: auto;"/>
          </figure>
          
          <p style="text-align: justify; margin-top: 0.2rem; margin-bottom: 1rem; font-style: italic; color: #666; font-size: 0.9em;">
            <strong>Figure 2:</strong> Trajectory data verification and force augmentation. (a,b) After generating initial grasp poses, the set of target directions along which the object can be manipulated is pruned using physics simulation, and the fingertips' trajectories are computed by sampling target object poses. (c) The contact force is synthesized as a surrogate for the finger’s penetration depth into the object, and we augment the trajectories with diverse forces.
          </p>

          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/data_traj.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column">
        <h2 class="title is-4">Human Demonstrations with Tactile</h2>
        <div class="content">            
            <figure class="image" style="margin-bottom: 0;">
              <img src="./static/images/human_tactile.png"
                  alt="Human tactile Demonstrations overview"
                  style="width: 100%; height: auto;"/>
            </figure>
            
            <p style="text-align: justify; margin-top: 0.2rem; margin-bottom: 1rem; font-style: italic; color: #666; font-size: 0.9em;">
              <strong>Figure 3:</strong> A wearable device for collecting real tactile-force data. The collected force during manipulation is characterized by the displacement of markers within the vision-based tactile sensor.
            </p>

            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/data_human.mp4"
                      type="video/mp4">
            </video>
        </div>
      </div>
      
    </div>
  <div class="columns is-centered">
      <div class="column is-full-width">
        <br> <h2 class="title is-4">Tactile Sensor</h2>
        <div class="content has-text-justified">
          <p>
            The tactile sensor is based on the <a href="https://peilin-666.github.io/projects/PP-Tac/" target="_blank" rel="noopener noreferrer">PP-Tac</a> design, which has been <a href="https://github.com/bigai-ai/PP-Tac/tree/main" target="_blank" rel="noopener noreferrer">open sourced</a>.
            We add some markers on the gel surface to track the contact deformation.
        </div>
<video id="sensor" autoplay controls muted loop playsinline 
           style="width: 60%; height: auto; display: block; margin: 0 auto;">
      <source src="./static/videos/sensor.mp4" type="video/mp4">
    </video>
      </div>
    </div>
  </div>
</section>

   
<!-- 
<section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        
        <h2 class="title is-3 has-text-centered">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
</section> -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{DexMove_ICLR2026,
  title={DexMove: Learning Tactile-Guided Non-Prehensile Manipulation with Dexterous Hands},
  author={Pei, Lin and Yuzhe, Huang and Wanlin, Li and Chenxi, Xiao and Ziyuan, Jiao},
  booktitle={The Fourteenth International Conference on Learning Representations},
  year={2026},
  url={https://openreview.net/forum?id=dT3ZciXvNX}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website borrows the source code of <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> .
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
