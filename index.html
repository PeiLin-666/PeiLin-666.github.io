<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />
    <title> Pei Lin </title>
</head>

<body>
    <div style="width:1000px;margin: 0px auto;">
        <header id="header" width="400px" style="display:flex;justify-content: space-around;">
            <a href="#profile-intro">Home</a>
            <a href="#updates">Updates</a>
            <a href="#research">Research</a>
            <!-- <a href="#projects">Projects</a> -->
        </header>
        <div id="profile">
            <div id="profile-pic">
                <img src="assets/images/profile.jpg" />
            </div>
            <div id="profile-intro">
                <div id="profile-name">Pei Lin 林沛</div>
                <div id="profile-email">linpei@bigai.ai<br> 
                       linpei2024@shanghaitech.edu.cn 
                </div>
                <p>
                    I'm a Computer Science PhD student at Beijing Institute for General Artificial Intelligence (BIGAI) and ShanghaiTech University. 
                    I'm fortunate to be advised by Prof. <a href="https://sites.google.com/g.ucla.edu/zyjiao/home">Ziyuan Jiao</a> and Prof. <a href="https://rim-laboratory.github.io/">Chenxi Xiao</a>.
                </p>
                <p>
                    My current research centers on dexterous robotic hands equipped with tactile perception. 
                    I aim to enable these dexterous robotic hands to perform more complex and higher-level maneuvers by endowing them with tactile capabilities.
                </p>
                <p>
                    In my previous research, I have primarily focused on human body motion,
                    particularly interactive hand motion, which has greatly contributed to my current work. 
                    I feel incredibly fortunate to have had the guidance of Prof. <a href="http://www.xu-lan.com/">Lan Xu</a>, Prof. <a href="https://www.yu-jingyi.com/">Jingyi Yi</a> and  Prof. <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a> during my master's studies.
                </p>
                
                <div>
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=wR-6tm4AAAAJ">
                      Google Scholar
                    </a>
                    </a>
                    /
                    <a href="https://github.com/PeiLin-666">
                        Github
                    </a>
                    </a>
                </div>
            </div>
            <div style="clear: both;"></div>
        </div>
        <div class="section" id="updates">
            <h1>Updates</h1>
            <ul>
                <li> <b>Mar 2025</b> PP-Tac is accdepted to ICLR 7th Robot Learning Workshop as a oral paper, see you in Singapore!
                <li> <b>Dec 2024</b> One papers accepted to AAAI 2025
                <li> <b>Jul 2024</b> I started my PhD at BIGAI
            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="research">
            <h1>Research</h1>

            <div>
              <a href="https://peilin-666.github.io/projects/PP-Tac/" style="height: 14em;" class="research-thumb">
                  <img width="320" src="assets/images/PP_Tac.png" alt="">
              </a>
              <a href="https://peilin-666.github.io/projects/PP-Tac/" class="research-proj-title">
                PP-Tac: Paper Picking Using Omnidirectional Tactile Feedback in Dexterous Robotic Hands
              </a>
              <p>
                  <b>Pei Lin</b>*,
                  <a style="color:#000;">Yuzhe Huang</a>*,
                  <a style="color:#000;">Wanlin Li</a>*,
                  <a style="color:#000;">Jianpeng Ma</a>,
                  <a style="color:#000;">Chenxi Xiao</a>†,
                  <a style="color:#000;">Ziyuan Jiao</a>†,
                  <br>7th Robot Learning Workshop in ICLR 2025, <strong > oral</strong><br>
                  <a href="https://peilin-666.github.io/projects/PP-Tac/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://peilin-666.github.io/projects/PP-Tac/">ArXiv(Coming Soon)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                  <a href="https://peilin-666.github.io/projects/PP-Tac/">Code(Coming Soon)
              </p>
          </div>


            <div>
                <a href="https://handdiffuse.github.io" style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/HandDiffuse.png" alt="">
                </a>
                <a href="https://handdiffuse.github.io" class="research-proj-title">
                    HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models
                </a>
                <p>
                    <b>Pei Lin</b>*,
                    <a style="color:#000;">Sihang Xu</a>,
                    <a style="color:#000;">Hongdi Yang</a>,
                    <a style="color:#000;">Yiran Liu</a>,
                    <a style="color:#000;">Xin Chen</a>,
                    <a style="color:#000;">Jingya Wang</a>,
                    <a style="color:#000;">Jingyi Yu</a>,
                    <a style="color:#000;">Lan Xu</a>,
                    <br>Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2025)<br>
                    <a href="https://handdiffuse.github.io">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/abs/2312.04867">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://handdiffuse.github.io">Dataset(Coming Soon)
                </p>
            </div>

            <div>
                <a href="https://zhaofuq.github.io/humannerf/" style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/humannerf.png" alt="">
                </a>
                <a href="https://zhaofuq.github.io/humannerf/" class="research-proj-title">
                    HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs
                </a>
                <p>
                    <a style="color:#000;">Fuqiang Zhao</a>,
                    <a style="color:#000;">Wei Yang</a>,
                    <a style="color:#000;">Jiakai Zhang</a>,
                    <b>Pei Lin</b>,
                    <a style="color:#000;">Yingliang Zhang</a>,
                    <a style="color:#000;">Jingyi Yu</a>,
                    <a style="color:#000;">Lan Xu</a>,
                    <br>Conference on Computer Vision and Pattern Recognition (CVPR 2022)<br>
                    <a href="https://zhaofuq.github.io/humannerf/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/pdf/2112.02789.pdf">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://zhaofuq.github.io/humannerf/">Code
                </p>
            </div>


            <div>
                <a href="https://nowheretrix.github.io/neuralhumanFVV/" style="height: 14em;" class="research-thumb">
                    <video width="320" height="auto" autoplay loop muted>
                        <source src="assets/images/NeuralHumanFVV.mp4" type="video/mp4">
                    </video>
                </a>
                <a href="https://nowheretrix.github.io/neuralhumanFVV/" class="research-proj-title">
                    NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras
                </a>
                <p>
                    <a style="color:#000;">Xin Suo</a>,
                    <a style="color:#000;">Yuheng Jiang</a>,
                    <b>Pei Lin</b>,
                    <a style="color:#000;">Yingliang Zhang</a>,
                    <a style="color:#000;">Minye Wu</a>,
                    <a style="color:#000;">Kaiwen Guo</a>,
                    <a style="color:#000;">Lan Xu</a>,
                    <br>Conference on Computer Vision and Pattern Recognition (CVPR 2021)<br>
                    <a href="https://nowheretrix.github.io/neuralhumanFVV/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Suo_NeuralHumanFVV_Real-Time_Neural_Volumetric_Human_Performance_Rendering_Using_RGB_Cameras_CVPR_2021_paper.html">ArXiv</a>
                </p>
            </div>



            <div>
                <a href="https://sunshinnnn.github.io/HOI-FVV" style="height: 14em;" class="research-thumb">
                    <img width="320" src="assets/images/HOI.png" alt="">
                </a>
                <a href="https://umi-gripper.github.io/" class="research-proj-title">
                    Neural Free-Viewpoint Performance Rendering under Complex Human-object Interactions
                </a>
                <p>
                    <a style="color:#000;">Guoxing Sun</a>,
                    <a style="color:#000;">Xin Chen</a>,
                    <a style="color:#000;">Yizhang Chen</a>,
                    <a style="color:#000;">Anqi Pang</a>,
                    <b>Pei Lin</b>,
                    <a style="color:#000;">Yuheng Jiang</a>,
                    <a style="color:#000;">Lan Xu</a>,
                    <a style="color:#000;">Jingya Wang</a>,
                    <a style="color:#000;">Jingyi Yu</a>,
                    <br>ACM Multimedia (ACM MM 2021 <strong >oral </strong>)<br>
                    <a href="https://sunshinnnn.github.io/HOI-FVV">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/abs/2312.04867">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>



</body>
<footer style="text-align: center;">
  © 2025 Pei Lin. All rights reserved.<br>
    Template designed by <a href="https://cheng-chi.github.io" target="_blank">Cheng Chi</a>.
</footer>
</html>
